{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langsmith langgraph langgraph-cli langchain_google_genai langgraph-checkpoint-postgres psycopg psycopg-pool psycopg-binary tavily-python langchain_community python-dotenv langchain-core==0.3.21 pydantic==2.8.2 pydantic-settings==2.5.2 packaging==23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal, Dict, Union, Optional\n",
    "from psycopg_pool import ConnectionPool\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, RemoveMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import MessagesState\n",
    "from io import BytesIO\n",
    "from pydantic import BaseModel\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "NEON_DB_URI = os.getenv(\"NEON_DB_URI\")\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"]= os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"chatbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Postgres DB\n",
    "\n",
    "# # Connection pool for efficient database access\n",
    "# connection_kwargs = {\"autocommit\": True, \"prepare_threshold\": 0}\n",
    "\n",
    "# # Create a persistent connection pool\n",
    "# pool = ConnectionPool(conninfo=NEON_DB_URI, max_size=50, kwargs=connection_kwargs)\n",
    "\n",
    "# # Initialize PostgresSaver checkpointer\n",
    "# checkpointer = PostgresSaver(pool)\n",
    "# checkpointer.setup()  # Ensure database tables are set up\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# State\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "    ask_human: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RequestAssistance(BaseModel):\n",
    "    \"\"\"Escalate the conversation to an expert Doctor. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
    "\n",
    "    To use this function, relay the user's 'request' so the expert Doctor can provide the right guidance.\n",
    "    \"\"\"\n",
    "\n",
    "    request: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tavily Search Tool Node\n",
    "\n",
    "search_tool = TavilySearchResults(max_results=2, search_depth=\"advanced\")\n",
    "tools = [search_tool]\n",
    "\n",
    "tool_node = ToolNode(tools=[search_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "model = model.bind_tools(tools + [RequestAssistance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def create_response(response: str, ai_message: AIMessage):\n",
    "    return ToolMessage(\n",
    "        content=response,\n",
    "        tool_call_id=ai_message.tool_calls[0][\"id\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def doctor_node(state: State):\n",
    "    new_messages = []\n",
    "    if not isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        # Typically, the user will have updated the state during the interrupt.\n",
    "        # If they choose not to, we will include a placeholder ToolMessage to\n",
    "        # let the LLM continue.\n",
    "        new_messages.append(\n",
    "            create_response(\"No response from Doctor.\", state[\"messages\"][-1])\n",
    "        )\n",
    "    return {\n",
    "        # Append the new messages\n",
    "        \"messages\": new_messages,\n",
    "        # Unset the flag\n",
    "        \"ask_doctor\": False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Summarization\n",
    "\n",
    "def summarize_conversation(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Summarizes the conversation if the number of messages exceeds 6 messages.\n",
    "    \n",
    "    Args:\n",
    "        state (State): The current conversation state.\n",
    "        model (object): The model to use for summarization.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, object]: A dictionary containing updated messages.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create summarization prompt based on whether there is an existing summary\n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is the summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add the summarization prompt to the conversation history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # Save summary and truncate old messages\n",
    "    state[\"summary\"] = response.content\n",
    "    state[\"messages\"] = state[\"messages\"][-2:]  # Keep only the last 2 messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Conditional Function\n",
    "\n",
    "def select_next_node(state: State) -> Union[Literal[\"tools\", \"summarize\", \"doctor\"], str]:\n",
    "    # Check if the conversation requires human intervention\n",
    "    if state[\"ask_doctor\"]:\n",
    "        return \"doctor\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If there are more than six messages, route to \"summarize_conversation\"\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize\"\n",
    "    \n",
    "    # If the LLM makes a tool call, route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Otherwise, route to \"final\" or end\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Invoke Messages\n",
    "\n",
    "def call_model(state: State) -> Dict[str, object]:\n",
    "\n",
    "    # Ensure state contains 'messages'\n",
    "    if \"messages\" not in state:\n",
    "        raise ValueError(\"State must contain a 'messages' key.\")\n",
    "    \n",
    "    # Initialize messages from the state\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Check if a summary exists and prepend it as a system message if present\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "\n",
    "    # Safely invoke the model\n",
    "    try:\n",
    "        response = model.invoke(messages)\n",
    "        ask_doctor = False\n",
    "\n",
    "        if (\n",
    "            response.tool_calls\n",
    "            and response.tool_calls[0][\"name\"] == RequestAssistance.__name__\n",
    "        ):\n",
    "            ask_doctor = True\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error invoking the model: {e}\")\n",
    "\n",
    "    # Append the response to messages\n",
    "    messages.append(response)\n",
    "\n",
    "    # Return the updated state with messages\n",
    "    return {\"messages\": messages[-1], \"ask_doctor\": ask_doctor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Build Graph\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"agent\", call_model)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.add_node(\"doctor\", doctor_node)\n",
    "builder.add_node(\"summarize\", summarize_conversation)\n",
    "\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "builder.add_edge(\"doctor\", \"agent\")\n",
    "builder.add_edge(\"summarize\", END)\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    select_next_node,\n",
    "    {\"summarize\": \"summarize\", \"doctor\": \"doctor\", \"tools\": \"tools\", END: END},\n",
    ")\n",
    "\n",
    "graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # We interrupt before 'human' here instead.\n",
    "    interrupt_before=[\"doctor\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "user_input = \"I need some expert guidance for building this AI agent. Could you request assistance for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
